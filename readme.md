# Transformer

the implementation of paper [attention is all you need](https://arxiv.org/abs/1706.03762)
